#!/usr/bin/env python3
"""
boe_downloader_eli_optimized.py

Downloader/ingestor para BOE (orientado a ELI) con:

- Descarga del catálogo de legislación consolidada (solo items con url_eli) y descarga del XML consolidado por doc_id.
- Descarga del sumario BOE diario (AAAAMMDD) y descarga del XML de cada item (url_xml).
- Caché HTTP condicional (ETag / Last-Modified) -> 304 no reescribe.
- Manifest JSONL (index/) con run_id, status, hashes, etc.
- Concurrencia fija o AUTO (AIMD) para adaptarse a rate limiting / saturación.
- Reintentos con backoff + decorrelated jitter, respetando Retry-After.
- Barra de progreso Rich con métricas (ok/errors/429, bytes, concurrency, etc.).

Uso (help):
  python3 boe_downloader_eli_optimized.py -h

Notas:
- Los payloads se guardan como .bin aunque sean XML/JSON para tratarlo como "bytes" opacos.
  El Content-Type real queda en meta/*.json y en el manifest.
"""

from __future__ import annotations

import argparse
import asyncio
import email.utils
import hashlib
import json
import os
import random
import re
import time
from dataclasses import dataclass, asdict
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple
from urllib.parse import urljoin

import aiofiles
import aiofiles.os
import aiohttp
from aiohttp import ClientSession

from rich.console import Console
from rich.live import Live
from rich.progress import (
    Progress,
    BarColumn,
    SpinnerColumn,
    TextColumn,
    TimeElapsedColumn,
    TimeRemainingColumn,
    MofNCompleteColumn,
)
from rich.table import Table
from rich.panel import Panel

BASE = "https://www.boe.es"
SUMARIO_API = f"{BASE}/datosabiertos/api/boe/sumario"  # + /{fecha}
LEGIS_API = f"{BASE}/datosabiertos/api/legislacion-consolidada"

DEFAULT_STORE = "./boe_store"
DEFAULT_TIMEOUT_S = 90
DEFAULT_RETRIES = 6
DEFAULT_BASE_DELAY = 0.5
DEFAULT_CAP_DELAY = 20.0
DEFAULT_CONCURRENCY_START = 10
DEFAULT_CONCURRENCY_MAX = 25

# -----------------------------
# Persistencia (disco)
# -----------------------------

@dataclass
class StoredMeta:
    etag: Optional[str] = None
    last_modified: Optional[str] = None
    sha256: Optional[str] = None
    content_type: Optional[str] = None
    fetched_at_utc: Optional[str] = None


def sha256_bytes(b: bytes) -> str:
    return hashlib.sha256(b).hexdigest()


def ensure_dirs(store_dir: str) -> None:
    os.makedirs(os.path.join(store_dir, "data"), exist_ok=True)
    os.makedirs(os.path.join(store_dir, "meta"), exist_ok=True)
    os.makedirs(os.path.join(store_dir, "index"), exist_ok=True)


def url_key(url: str) -> str:
    return hashlib.sha1(url.encode("utf-8")).hexdigest()


def paths_for_url(store_dir: str, url: str) -> Tuple[str, str]:
    k = url_key(url)
    data_path = os.path.join(store_dir, "data", f"{k}.bin")
    meta_path = os.path.join(store_dir, "meta", f"{k}.json")
    return data_path, meta_path


def index_path(store_dir: str, name: str) -> str:
    return os.path.join(store_dir, "index", name)


async def load_meta(meta_path: str) -> StoredMeta:
    try:
        if not await aiofiles.os.path.exists(meta_path):
            return StoredMeta()
        async with aiofiles.open(meta_path, "r", encoding="utf-8") as f:
            return StoredMeta(**json.loads(await f.read()))
    except Exception:
        return StoredMeta()


async def save_meta(meta_path: str, meta: StoredMeta) -> None:
    os.makedirs(os.path.dirname(meta_path), exist_ok=True)
    async with aiofiles.open(meta_path, "w", encoding="utf-8") as f:
        await f.write(json.dumps(asdict(meta), ensure_ascii=False, indent=2))


# -----------------------------
# Errores y Retry-After
# -----------------------------

class RetryableHTTPError(RuntimeError):
    def __init__(self, status: int, url: str, retry_after_s: float | None = None, msg: str | None = None):
        super().__init__(msg or f"HTTP {status} retryable for {url}")
        self.status = status
        self.url = url
        self.retry_after_s = retry_after_s


class NonRetryableHTTPError(RuntimeError):
    def __init__(self, status: int, url: str, msg: str | None = None):
        super().__init__(msg or f"HTTP {status} for {url}")
        self.status = status
        self.url = url


def parse_retry_after(value: str) -> float | None:
    value = (value or "").strip()
    if not value:
        return None
    if value.isdigit():
        return float(value)
    try:
        dt = email.utils.parsedate_to_datetime(value)
        if dt is None:
            return None
        if dt.tzinfo is None:
            now = datetime.utcnow()
            return max(0.0, (dt - now).total_seconds())
        now = datetime.now(dt.tzinfo)
        return max(0.0, (dt - now).total_seconds())
    except Exception:
        return None


# -----------------------------
# Métricas + Concurrencia AUTO (AIMD)
# -----------------------------

class RunStats:
    def __init__(self) -> None:
        self._lock = asyncio.Lock()
        self.reset_window()
        self.total_done = 0
        self.total_ok = 0
        self.total_skipped_304 = 0
        self.total_errors = 0
        self.total_http429 = 0
        self.total_http5xx = 0
        self.total_bytes = 0
        self.max_concurrency_reached = 0

    def reset_window(self) -> None:
        self.win_ok = 0
        self.win_err = 0
        self.win_429 = 0
        self.win_5xx = 0
        self.win_timeouts = 0
        self.win_lat: list[float] = []
        self.win_started = time.monotonic()

    async def record(self, *, status: int | None, latency_s: float, nbytes: int, timeout: bool = False) -> None:
        async with self._lock:
            self.total_done += 1
            if status == 304:
                self.total_skipped_304 += 1
            if status is not None and 200 <= status < 300:
                self.total_ok += 1
                self.win_ok += 1
            else:
                self.total_errors += 1
                self.win_err += 1
            if status == 429:
                self.total_http429 += 1
                self.win_429 += 1
            if status is not None and status >= 500:
                self.total_http5xx += 1
                self.win_5xx += 1
            if timeout:
                self.win_timeouts += 1
            self.total_bytes += max(0, nbytes)
            self.win_lat.append(max(0.0, latency_s))

    async def snapshot_window(self) -> dict[str, float]:
        async with self._lock:
            dur = max(0.001, time.monotonic() - self.win_started)
            avg_lat = (sum(self.win_lat) / len(self.win_lat)) if self.win_lat else 0.0
            rps = (self.win_ok + self.win_err) / dur
            snap = {
                "duration_s": dur,
                "ok": self.win_ok,
                "err": self.win_err,
                "http429": self.win_429,
                "http5xx": self.win_5xx,
                "timeouts": self.win_timeouts,
                "avg_latency_s": avg_lat,
                "rps": rps,
            }
            self.reset_window()
            return snap


class AdaptiveLimiter:
    """
    Semáforo ajustable mediante 'reservas' (permite bajar/subir target en caliente).
    """
    def __init__(self, max_limit: int, initial: int) -> None:
        self.max_limit = max(1, int(max_limit))
        self._sem = asyncio.Semaphore(self.max_limit)
        self._reserved = 0
        self._target = max(1, min(self.max_limit, int(initial)))
        self._lock = asyncio.Lock()

    async def initialize(self) -> None:
        await self.set_target(self._target)

    async def _set_reserved(self, desired_reserved: int) -> None:
        desired_reserved = max(0, min(self.max_limit - 1, int(desired_reserved)))
        while self._reserved < desired_reserved:
            await self._sem.acquire()
            self._reserved += 1
        while self._reserved > desired_reserved:
            self._sem.release()
            self._reserved -= 1

    async def set_target(self, target: int) -> int:
        target = max(1, min(self.max_limit, int(target)))
        async with self._lock:
            self._target = target
            desired_reserved = self.max_limit - self._target
        await self._set_reserved(desired_reserved)
        return self._target

    async def get_target(self) -> int:
        async with self._lock:
            return self._target

    async def acquire(self) -> None:
        await self._sem.acquire()

    def release(self) -> None:
        self._sem.release()


async def autotune_concurrency(limiter: AdaptiveLimiter, stats: RunStats, *, start: int, max_limit: int, interval_s: float = 5.0) -> None:
    baseline: float | None = None
    await limiter.set_target(start)
    while True:
        await asyncio.sleep(interval_s)
        snap = await stats.snapshot_window()
        cur = await limiter.get_target()

        if snap["rps"] > 0 and baseline is None and snap["avg_latency_s"] > 0:
            baseline = snap["avg_latency_s"]

        congested = (snap["http429"] > 0) or (snap["http5xx"] > 0) or (snap["timeouts"] > 0)
        if baseline is not None and snap["avg_latency_s"] > 0 and snap["err"] > 0:
            if snap["avg_latency_s"] >= 2.0 * baseline:
                congested = True

        if congested:
            new = max(1, int(cur * 0.7))
            await limiter.set_target(new)
        else:
            if snap["rps"] > 0 and cur < max_limit:
                await limiter.set_target(cur + 1)

        tgt = await limiter.get_target()
        stats.max_concurrency_reached = max(stats.max_concurrency_reached, tgt)


# -----------------------------
# HTTP fetch con caché + reintentos (decorrelated jitter)
# -----------------------------

async def fetch_with_cache(
    session: ClientSession,
    store_dir: str,
    url: str,
    accept: str,
    *,
    retries: int,
    base_delay_s: float,
    cap_delay_s: float,
    jitter: str,
) -> Tuple[Optional[bytes], StoredMeta, int]:
    data_path, meta_path = paths_for_url(store_dir, url)
    meta = await load_meta(meta_path)

    headers = {"Accept": accept}
    if meta.etag:
        headers["If-None-Match"] = meta.etag
    if meta.last_modified:
        headers["If-Modified-Since"] = meta.last_modified

    use_decorrelated = (jitter == "decorrelated")
    sleep_s = base_delay_s
    last_exc: Exception | None = None

    for attempt in range(1, retries + 1):
        try:
            async with session.get(url, headers=headers) as resp:
                status = resp.status

                if status == 304:
                    return None, meta, status

                if status >= 400:
                    body = await resp.read()
                    if status in (429, 503) or status >= 500:
                        ra = parse_retry_after(resp.headers.get("Retry-After", ""))
                        raise RetryableHTTPError(status=status, url=url, retry_after_s=ra, msg=f"HTTP {status} retryable: {body[:200]!r}")
                    raise NonRetryableHTTPError(status=status, url=url, msg=f"HTTP {status}: {body[:200]!r}")

                content = await resp.read()
                meta.etag = resp.headers.get("ETag")
                meta.last_modified = resp.headers.get("Last-Modified")
                meta.content_type = resp.headers.get("Content-Type")
                meta.sha256 = sha256_bytes(content)
                meta.fetched_at_utc = datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ")

                os.makedirs(os.path.dirname(data_path), exist_ok=True)
                async with aiofiles.open(data_path, "wb") as f:
                    await f.write(content)
                await save_meta(meta_path, meta)
                return content, meta, status

        except NonRetryableHTTPError:
            raise
        except RetryableHTTPError as e:
            last_exc = e
            if attempt >= retries:
                break
            if e.retry_after_s and e.retry_after_s > 0:
                await asyncio.sleep(min(cap_delay_s, e.retry_after_s))
                continue
            # backoff + jitter
            if use_decorrelated:
                upper = min(cap_delay_s, sleep_s * 3.0)
                sleep_s = random.uniform(base_delay_s, upper)
            else:
                backoff = min(cap_delay_s, base_delay_s * (2 ** (attempt - 1)))
                sleep_s = random.uniform(0, backoff)
            await asyncio.sleep(sleep_s)
        except (aiohttp.ClientError, asyncio.TimeoutError) as e:
            last_exc = e
            if attempt >= retries:
                break
            if use_decorrelated:
                upper = min(cap_delay_s, sleep_s * 3.0)
                sleep_s = random.uniform(base_delay_s, upper)
            else:
                backoff = min(cap_delay_s, base_delay_s * (2 ** (attempt - 1)))
                sleep_s = random.uniform(0, backoff)
            await asyncio.sleep(sleep_s)
        except Exception as e:
            last_exc = e
            if attempt >= retries:
                break
            await asyncio.sleep(min(cap_delay_s, base_delay_s * attempt))

    raise RuntimeError(f"Failed fetching {url} after {retries} retries. Last error: {last_exc}")


# -----------------------------
# BOE helpers
# -----------------------------

def is_eli_url(s: str | None) -> bool:
    return bool(s) and s.strip().startswith(f"{BASE}/eli/")


def build_consolidated_id_url(doc_id: str, *, part: str) -> str:
    # API: /datosabiertos/api/legislacion-consolidada/id/{identificador}[/{part}]
    base = f"{LEGIS_API}/id/{doc_id}"
    if part and part != "full":
        return f"{base}/{part}"
    return base


async def get_consolidated_list_json(session: ClientSession, *, since_from: str | None, since_to: str | None, store_dir: str, retries: int, base_delay: float, cap_delay: float, jitter: str) -> List[Dict[str, Any]]:
    if since_from or since_to:
        params = []
        if since_from:
            params.append(f"from={since_from}")
        if since_to:
            params.append(f"to={since_to}")
        params.append("limit=-1")
        url = f"{LEGIS_API}?{'&'.join(params)}"
    else:
        url = f"{LEGIS_API}?limit=-1"

    content, _meta, status = await fetch_with_cache(
        session=session, store_dir=store_dir, url=url, accept="application/json",
        retries=retries, base_delay_s=base_delay, cap_delay_s=cap_delay, jitter=jitter
    )
    if content is None:
        raise RuntimeError("Catálogo consolidado devolvió 304 inesperado (sin caché previa).")
    if status >= 400:
        raise RuntimeError(f"Catálogo consolidado HTTP {status}")
    d = json.loads(content.decode("utf-8"))
    # el API devuelve {"data":[...]} o lista directa según versiones
    if isinstance(d, dict) and "data" in d and isinstance(d["data"], list):
        return d["data"]
    if isinstance(d, list):
        return d
    raise RuntimeError("Formato JSON inesperado en catálogo consolidado.")


async def get_sumario_xml(session: ClientSession, *, fecha: str, store_dir: str, retries: int, base_delay: float, cap_delay: float, jitter: str) -> bytes:
    url = f"{SUMARIO_API}/{fecha}"
    content, _meta, status = await fetch_with_cache(
        session=session, store_dir=store_dir, url=url, accept="application/xml",
        retries=retries, base_delay_s=base_delay, cap_delay_s=cap_delay, jitter=jitter
    )
    if content is None:
        # si ya existe, recargamos de disco
        data_path, _ = paths_for_url(store_dir, url)
        async with aiofiles.open(data_path, "rb") as f:
            return await f.read()
    if status >= 400:
        raise RuntimeError(f"Sumario HTTP {status}")
    return content


def extract_sumario_item_urls(xml_bytes: bytes) -> List[str]:
    # parsing minimal sin lxml (para mantener dependencias); usamos regex por estabilidad en este caso.
    # En el XML del sumario, url_xml suele aparecer como <url_xml>...</url_xml>
    txt = xml_bytes.decode("utf-8", errors="ignore")
    return [m.group(1).strip() for m in re.finditer(r"<url_xml>(.*?)</url_xml>", txt, flags=re.DOTALL)]


# -----------------------------
# UI Rich
# -----------------------------

def make_status_panel(*, run_id: str, cmd: str, stats: RunStats, concurrency: int) -> Panel:
    t = Table.grid(expand=True)
    t.add_column(justify="left")
    t.add_column(justify="right")
    t.add_row("run_id", run_id)
    t.add_row("cmd", cmd)
    t.add_row("concurrency(target)", str(concurrency))
    t.add_row("done", str(stats.total_done))
    t.add_row("ok", str(stats.total_ok))
    t.add_row("skipped_304", str(stats.total_skipped_304))
    t.add_row("errors", str(stats.total_errors))
    t.add_row("http_429", str(stats.total_http429))
    t.add_row("http_5xx", str(stats.total_http5xx))
    t.add_row("bytes", str(stats.total_bytes))
    t.add_row("max_concurrency", str(stats.max_concurrency_reached))
    return Panel(t, title="Estado", border_style="cyan")


# -----------------------------
# Pipelines
# -----------------------------

async def run_queue_download(
    *,
    session: ClientSession,
    store_dir: str,
    cmd: str,
    items: List[Dict[str, Any]],
    accept: str,
    manifest_file: str,
    limiter: AdaptiveLimiter,
    stats: RunStats,
    run_id: str,
    progress: bool,
    retries: int,
    base_delay: float,
    cap_delay: float,
    jitter: str,
) -> None:
    """
    items: cada elemento debe tener { "key": ..., "url": ..., ... } para manifest.
    """
    manifest_path = index_path(store_dir, manifest_file)
    manifest_lock = asyncio.Lock()

    async def append_manifest(obj: Dict[str, Any]) -> None:
        obj = dict(obj)
        obj["run_id"] = run_id
        obj["cmd"] = cmd
        obj["ts_utc"] = datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ")
        line = json.dumps(obj, ensure_ascii=False)
        async with manifest_lock:
            async with aiofiles.open(manifest_path, "a", encoding="utf-8") as f:
                await f.write(line + "\n")

    q: asyncio.Queue[Dict[str, Any]] = asyncio.Queue()
    for it in items:
        q.put_nowait(it)

    console = Console()
    prog = Progress(
        SpinnerColumn(),
        TextColumn("[bold]Descargando[/bold]"),
        BarColumn(),
        MofNCompleteColumn(),
        TextColumn("•"),
        TimeElapsedColumn(),
        TextColumn("•"),
        TimeRemainingColumn(),
        console=console,
    )
    task_id = prog.add_task("download", total=len(items))

    async def handle_one(it: Dict[str, Any]) -> None:
        url = it["url"]
        key = it.get("key")
        t0 = time.monotonic()
        status: int | None = None
        nbytes = 0
        timeout = False
        try:
            content, meta, status = await fetch_with_cache(
                session=session, store_dir=store_dir, url=url, accept=accept,
                retries=retries, base_delay_s=base_delay, cap_delay_s=cap_delay, jitter=jitter
            )
            if content is not None:
                nbytes = len(content)

            await append_manifest({
                "key": key,
                "url": url,
                "ok": (status is not None and status < 400),
                "status": status,
                "content_type": meta.content_type,
                "etag": meta.etag,
                "last_modified": meta.last_modified,
                "sha256": meta.sha256,
                "fetched_at_utc": meta.fetched_at_utc,
            })
        except NonRetryableHTTPError as e:
            status = e.status
            await append_manifest({"key": key, "url": url, "ok": False, "status": status, "error": str(e)})
        except asyncio.TimeoutError as e:
            timeout = True
            await append_manifest({"key": key, "url": url, "ok": False, "status": None, "error": f"timeout: {e}"})
        except Exception as e:
            await append_manifest({"key": key, "url": url, "ok": False, "status": status, "error": str(e)})
        finally:
            await stats.record(status=status, latency_s=(time.monotonic() - t0), nbytes=nbytes, timeout=timeout)
            prog.update(task_id, advance=1)

    async def worker() -> None:
        while True:
            try:
                it = q.get_nowait()
            except asyncio.QueueEmpty:
                return
            await limiter.acquire()
            try:
                await handle_one(it)
            finally:
                limiter.release()
                q.task_done()


    workers = [asyncio.create_task(worker()) for _ in range(limiter.max_limit)]

    if progress:
        # UI: progreso + panel estado en vivo
        with Live(console=console, refresh_per_second=8) as live:
            while not prog.finished:
                cur = await limiter.get_target()
                grid = Table.grid(padding=(0, 1))
                grid.add_row(
                    Panel.fit(prog.get_renderable(), title="Progreso", border_style="green"),
                    make_status_panel(run_id=run_id, cmd=cmd, stats=stats, concurrency=cur),
                )
                live.update(grid)
                await asyncio.sleep(0.3)

            cur = await limiter.get_target()
            grid = Table.grid(padding=(0, 1))
            grid.add_row(
                Panel.fit(prog.get_renderable(), title="Progreso", border_style="green"),
                make_status_panel(run_id=run_id, cmd=cmd, stats=stats, concurrency=cur),
            )
            live.update(grid)

    await q.join()

    for w in workers:
        w.cancel()
    await asyncio.gather(*workers, return_exceptions=True)


async def cmd_consolidada(
    session: ClientSession,
    *,
    store_dir: str,
    run_id: str,
    accept: str,
    part: str,
    manifest: str,
    since_from: str | None,
    since_to: str | None,
    progress: bool,
    limiter: AdaptiveLimiter,
    stats: RunStats,
    retries: int,
    base_delay: float,
    cap_delay: float,
    jitter: str,
    eli_list_file: str | None,
) -> None:
    items = await get_consolidated_list_json(
        session, since_from=since_from, since_to=since_to,
        store_dir=store_dir, retries=retries, base_delay=base_delay, cap_delay=cap_delay, jitter=jitter
    )

    # build targets (eli -> doc_id)
    if eli_list_file:
        async with aiofiles.open(eli_list_file, "r", encoding="utf-8") as f:
            wanted = {ln.strip() for ln in (await f.read()).splitlines() if ln.strip()}
    else:
        wanted = None

    targets: List[Dict[str, Any]] = []
    for it in items:
        doc_id = it.get("identificador")
        eli = it.get("url_eli")
        if not doc_id or not is_eli_url(eli):
            continue
        eli = eli.strip()
        if wanted is not None and eli not in wanted:
            continue
        url = build_consolidated_id_url(doc_id, part=part)
        targets.append({"key": eli, "doc_id": doc_id, "url": url})

    await run_queue_download(
        session=session, store_dir=store_dir, cmd="consolidada",
        items=targets, accept=accept, manifest_file=manifest,
        limiter=limiter, stats=stats, run_id=run_id, progress=progress,
        retries=retries, base_delay=base_delay, cap_delay=cap_delay, jitter=jitter
    )


async def cmd_sumario(
    session: ClientSession,
    *,
    store_dir: str,
    run_id: str,
    fecha: str,
    manifest: str,
    progress: bool,
    limiter: AdaptiveLimiter,
    stats: RunStats,
    retries: int,
    base_delay: float,
    cap_delay: float,
    jitter: str,
) -> None:
    if not re.fullmatch(r"\d{8}", fecha):
        raise ValueError("fecha debe tener formato AAAAMMDD")
    sumario_xml = await get_sumario_xml(session, fecha=fecha, store_dir=store_dir, retries=retries, base_delay=base_delay, cap_delay=cap_delay, jitter=jitter)
    urls = extract_sumario_item_urls(sumario_xml)
    targets = [{"key": u, "url": u} for u in urls]
    await run_queue_download(
        session=session, store_dir=store_dir, cmd="sumario",
        items=targets, accept="application/xml", manifest_file=manifest,
        limiter=limiter, stats=stats, run_id=run_id, progress=progress,
        retries=retries, base_delay=base_delay, cap_delay=cap_delay, jitter=jitter
    )


# -----------------------------
# CLI
# -----------------------------

def _parse_concurrency(value: str) -> str | int:
    v = value.strip().lower()
    if v in ("auto", "a"):
        return "auto"
    if v.isdigit():
        n = int(v)
        if n < 1:
            raise argparse.ArgumentTypeError("concurrency must be >= 1")
        return n
    raise argparse.ArgumentTypeError("concurrency must be an integer (e.g. 25) or 'auto'")


def build_arg_parser() -> argparse.ArgumentParser:
    p = argparse.ArgumentParser(
        prog="boe_downloader_eli_optimized.py",
        description="Descarga BOE orientada a ELI (consolidada y sumario) con caché y concurrencia auto.",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=(
            "Ejemplos:\n"
            "  python3 boe_downloader_eli_optimized.py --store ./boe_store --concurrency auto consolidada --since-from 20260101\n"
            "  python3 boe_downloader_eli_optimized.py --concurrency 20 sumario --fecha 20260104\n"
        ),
    )
    p.add_argument("--store", default=DEFAULT_STORE, metavar="DIR", help=f"Directorio base de almacenamiento. Default: {DEFAULT_STORE}")
    p.add_argument("--timeout", type=int, default=DEFAULT_TIMEOUT_S, metavar="S", help=f"Timeout total por request (segundos). Default: {DEFAULT_TIMEOUT_S}")
    p.add_argument("--retries", type=int, default=DEFAULT_RETRIES, metavar="N", help=f"Máximo reintentos por URL (429/5xx/transitorios). Default: {DEFAULT_RETRIES}")
    p.add_argument("--concurrency", type=_parse_concurrency, default="auto", metavar="N|auto", help="Concurrencia fija N o auto (AIMD). Default: auto")
    p.add_argument("--concurrency-start", type=int, default=DEFAULT_CONCURRENCY_START, metavar="N", help=f"Concurrencia inicial en auto. Default: {DEFAULT_CONCURRENCY_START}")
    p.add_argument("--concurrency-max", type=int, default=DEFAULT_CONCURRENCY_MAX, metavar="N", help=f"Techo de concurrencia en auto. Default: {DEFAULT_CONCURRENCY_MAX}")
    p.add_argument("--progress", action="store_true", help="Muestra barra de progreso y métricas en vivo (Rich).")
    p.add_argument("--jitter", choices=["decorrelated", "full"], default="decorrelated", help="Jitter para backoff de reintentos. Default: decorrelated")
    p.add_argument("--base-delay", type=float, default=DEFAULT_BASE_DELAY, metavar="S", help=f"Delay base para backoff (segundos). Default: {DEFAULT_BASE_DELAY}")
    p.add_argument("--cap-delay", type=float, default=DEFAULT_CAP_DELAY, metavar="S", help=f"Delay máximo para backoff (segundos). Default: {DEFAULT_CAP_DELAY}")

    sub = p.add_subparsers(dest="cmd", required=True)

    pc = sub.add_parser("consolidada", help="Descarga legislación consolidada SOLO con url_eli.", formatter_class=argparse.RawDescriptionHelpFormatter)
    pc.add_argument("--part", default="full", choices=["full", "metadatos", "analisis", "metadata-eli", "texto", "texto/indice"], help="Parte del documento. Default: full")
    pc.add_argument("--accept", default="application/xml", metavar="MIME", help="Cabecera Accept. Default: application/xml")
    pc.add_argument("--manifest", default="manifest_consolidada_eli.jsonl", metavar="FILE", help="Manifest JSONL en index/. Default: manifest_consolidada_eli.jsonl")
    pc.add_argument("--since-from", default=None, metavar="AAAAMMDD", help="Filtra por fecha actualización desde AAAAMMDD.")
    pc.add_argument("--since-to", default=None, metavar="AAAAMMDD", help="Filtra por fecha actualización hasta AAAAMMDD.")
    pc.add_argument("--eli-list", default=None, metavar="FILE", help="Archivo con una ELI por línea (descarga solo esas).")

    ps = sub.add_parser("sumario", help="Descarga sumario diario y XML de items.", formatter_class=argparse.RawDescriptionHelpFormatter)
    ps.add_argument("--fecha", required=True, metavar="AAAAMMDD", help="Fecha AAAAMMDD.")
    ps.add_argument("--manifest", default="manifest_sumario.jsonl", metavar="FILE", help="Manifest JSONL en index/. Default: manifest_sumario.jsonl")

    return p


async def amain(args: argparse.Namespace) -> None:
    store_dir = args.store
    ensure_dirs(store_dir)

    run_id = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ") + f"-{random.randint(1000,9999)}"
    console = Console()
    console.print(f"[bold]run_id:[/bold] {run_id}")

    timeout = aiohttp.ClientTimeout(total=int(args.timeout))

    # Conector: limit_per_host = concurrency_max (en auto) o concurrency fija
    if args.concurrency == "auto":
        max_limit = int(args.concurrency_max)
        start = int(args.concurrency_start)
    else:
        max_limit = int(args.concurrency)
        start = int(args.concurrency)

    connector = aiohttp.TCPConnector(limit=max_limit, limit_per_host=max_limit, ttl_dns_cache=300)

    stats = RunStats()
    limiter = AdaptiveLimiter(max_limit=max_limit, initial=start)
    await limiter.initialize()
    stats.max_concurrency_reached = max(stats.max_concurrency_reached, start)

    tuner_task: asyncio.Task | None = None
    if args.concurrency == "auto":
        tuner_task = asyncio.create_task(autotune_concurrency(limiter, stats, start=start, max_limit=max_limit, interval_s=5.0))

    try:
        async with aiohttp.ClientSession(timeout=timeout, connector=connector) as session:
            if args.cmd == "consolidada":
                await cmd_consolidada(
                    session,
                    store_dir=store_dir,
                    run_id=run_id,
                    accept=args.accept,
                    part=args.part,
                    manifest=args.manifest,
                    since_from=args.since_from,
                    since_to=args.since_to,
                    progress=args.progress,
                    limiter=limiter,
                    stats=stats,
                    retries=int(args.retries),
                    base_delay=float(args.base_delay),
                    cap_delay=float(args.cap_delay),
                    jitter=args.jitter,
                    eli_list_file=args.eli_list,
                )
            elif args.cmd == "sumario":
                await cmd_sumario(
                    session,
                    store_dir=store_dir,
                    run_id=run_id,
                    fecha=args.fecha,
                    manifest=args.manifest,
                    progress=args.progress,
                    limiter=limiter,
                    stats=stats,
                    retries=int(args.retries),
                    base_delay=float(args.base_delay),
                    cap_delay=float(args.cap_delay),
                    jitter=args.jitter,
                )
            else:
                raise RuntimeError(f"Comando no reconocido: {args.cmd}")

    finally:
        if tuner_task is not None:
            tuner_task.cancel()
            await asyncio.gather(tuner_task, return_exceptions=True)

    # Resumen final
    cur = await limiter.get_target()
    console.print(make_status_panel(run_id=run_id, cmd=args.cmd, stats=stats, concurrency=cur))


def main() -> None:
    parser = build_arg_parser()
    args = parser.parse_args()
    asyncio.run(amain(args))


if __name__ == "__main__":
    main()

